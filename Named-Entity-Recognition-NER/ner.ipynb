{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Required Dependencies","metadata":{}},{"cell_type":"code","source":"! pip install seqeval","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import (\n    AutoModelForTokenClassification, \n    TrainingArguments, \n    Trainer,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    EarlyStoppingCallback\n)\n\nfrom seqeval.metrics import classification_report\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:44:32.816031Z","iopub.execute_input":"2024-07-21T03:44:32.816868Z","iopub.status.idle":"2024-07-21T03:44:32.830672Z","shell.execute_reply.started":"2024-07-21T03:44:32.816824Z","shell.execute_reply":"2024-07-21T03:44:32.829860Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load CoNLL-2003 dataset\ndataset = load_dataset('conll2003', trust_remote_code=True)\n\n# Split dataset\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\ntest_dataset = dataset['test']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-21T02:48:14.059031Z","iopub.execute_input":"2024-07-21T02:48:14.059454Z","iopub.status.idle":"2024-07-21T02:48:27.501584Z","shell.execute_reply.started":"2024-07-21T02:48:14.059429Z","shell.execute_reply":"2024-07-21T02:48:27.500813Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c711c20c9d6349c48060f14e44fdfd92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3ad9eb6b4574927a14768ed3210e850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06064a06a48498581f4856eee499814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2319a69b9e42c8b906fde66aa1d286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab69892649d4e7c8a5afc213b94f14f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cb9f8b6fc942999d671a8bf4a2c197"}},"metadata":{}}]},{"cell_type":"code","source":"ner_tags = dataset[\"train\"].features[\"ner_tags\"].feature.names\nprint(\"NER Tags: \", ner_tags)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:29:24.772942Z","iopub.execute_input":"2024-07-21T03:29:24.773215Z","iopub.status.idle":"2024-07-21T03:29:24.778692Z","shell.execute_reply.started":"2024-07-21T03:29:24.773187Z","shell.execute_reply":"2024-07-21T03:29:24.777857Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"NER Tags:  ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","output_type":"stream"}]},{"cell_type":"code","source":"# visualizing the data\noriginal_sample = dataset['train'][0]\nprint(\"Sample Tokens\" , original_sample['tokens'])\nprint(\"Sample Tags Indecies\" , original_sample['ner_tags'])\n\nmapped_ner_tags = [ner_tags[index] for index in original_sample['ner_tags']]\nprint(\"Sample NER Tags\" , mapped_ner_tags)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:03:47.571162Z","iopub.execute_input":"2024-07-21T03:03:47.571569Z","iopub.status.idle":"2024-07-21T03:03:47.578796Z","shell.execute_reply.started":"2024-07-21T03:03:47.571540Z","shell.execute_reply":"2024-07-21T03:03:47.577766Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Sample Tokens ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\nSample Tags Indecies [3, 0, 7, 0, 0, 0, 7, 0, 0]\nSample NER Tags ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## NER Tokenization","metadata":{}},{"cell_type":"code","source":"# Tokenizes input examples and aligns the NER tags with the tokenized inputs.\ndef tokenize_and_align_labels(examples, tokenizer):\n    \n    # Tokenize the input tokens\n    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n    labels = []\n    \n    # Loop through each example and its corresponding NER tags\n    for i, label in enumerate(examples[f'ner_tags']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        \n        # Get the word IDs from the tokenized inputs for the current example\n        for word_idx in word_ids:\n            if word_idx is None:\n                # If word_idx is None, it's a special token (e.g., [CLS], [SEP]), so we ignore it by setting -100\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                # If the token is part of the same word (sub-word token)\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:41:44.065304Z","iopub.execute_input":"2024-07-21T03:41:44.065965Z","iopub.status.idle":"2024-07-21T03:41:44.072776Z","shell.execute_reply.started":"2024-07-21T03:41:44.065930Z","shell.execute_reply":"2024-07-21T03:41:44.071854Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"metric = load_metric(\"seqeval\", trust_remote_code=True)\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=-1)\n\n    true_labels = [\n        [id2label[str(l)] for l in label if l != -100]\n        for label in labels\n    ]\n    true_predictions = [\n        [id2label[str(p)] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:45:02.436517Z","iopub.execute_input":"2024-07-21T03:45:02.437252Z","iopub.status.idle":"2024-07-21T03:45:03.170139Z","shell.execute_reply.started":"2024-07-21T03:45:02.437220Z","shell.execute_reply":"2024-07-21T03:45:03.169342Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# setting id2labels for easier visulaization during evaluation\nid2label = {str(i): label for i, label in enumerate(ner_tags)}\nlabel2id = {v: k for k, v in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:43:58.191158Z","iopub.execute_input":"2024-07-21T03:43:58.191777Z","iopub.status.idle":"2024-07-21T03:43:58.196590Z","shell.execute_reply.started":"2024-07-21T03:43:58.191743Z","shell.execute_reply":"2024-07-21T03:43:58.195602Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"markdown","source":"## BERT Tokenization","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\nbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n\n# Tokenize datasets for BERT\ntrain_dataset_bert = train_dataset.map(lambda examples: tokenize_and_align_labels(examples, bert_tokenizer), batched=True)\nval_dataset_bert = val_dataset.map(lambda examples: tokenize_and_align_labels(examples, bert_tokenizer), batched=True)\ntest_dataset_bert = test_dataset.map(lambda examples: tokenize_and_align_labels(examples, bert_tokenizer), batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:41:47.156384Z","iopub.execute_input":"2024-07-21T03:41:47.156763Z","iopub.status.idle":"2024-07-21T03:41:49.960693Z","shell.execute_reply.started":"2024-07-21T03:41:47.156733Z","shell.execute_reply":"2024-07-21T03:41:49.959791Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88075e70290a4283bed140b9fb46ba8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c8e07e896b4e1fb9b9f2fd10290367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dab5ec1674a4c3dbc7360f1b58155ed"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset_bert","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:42:03.263195Z","iopub.execute_input":"2024-07-21T03:42:03.264100Z","iopub.status.idle":"2024-07-21T03:42:03.269734Z","shell.execute_reply.started":"2024-07-21T03:42:03.264063Z","shell.execute_reply":"2024-07-21T03:42:03.268814Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 14041\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(bert_tokenizer.convert_ids_to_tokens(train_dataset_bert[0]['input_ids']))\nprint(train_dataset_bert[0]['labels'])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:42:03.720449Z","iopub.execute_input":"2024-07-21T03:42:03.721261Z","iopub.status.idle":"2024-07-21T03:42:03.727843Z","shell.execute_reply.started":"2024-07-21T03:42:03.721228Z","shell.execute_reply":"2024-07-21T03:42:03.726859Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n[-100, 3, 0, 7, 0, 0, 0, 7, 0, -100, 0, -100]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT Model & Training Arguments","metadata":{}},{"cell_type":"code","source":"# Load DistilBERT model\nbert_model = AutoModelForTokenClassification.from_pretrained('distilbert-base-cased', \n                                                             num_labels=len(ner_tags),\n                                                             id2label=id2label,\n                                                             label2id=label2id\n                                                            )","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:43:58.444864Z","iopub.execute_input":"2024-07-21T03:43:58.445750Z","iopub.status.idle":"2024-07-21T03:43:58.809970Z","shell.execute_reply.started":"2024-07-21T03:43:58.445711Z","shell.execute_reply":"2024-07-21T03:43:58.809213Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define training arguments with early stopping\nbert_training_args = TrainingArguments(\n    output_dir='./bert',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:45:37.735070Z","iopub.execute_input":"2024-07-21T03:45:37.735964Z","iopub.status.idle":"2024-07-21T03:45:37.766902Z","shell.execute_reply.started":"2024-07-21T03:45:37.735926Z","shell.execute_reply":"2024-07-21T03:45:37.766017Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_data_collator = DataCollatorForTokenClassification(tokenizer=bert_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:42:07.230626Z","iopub.execute_input":"2024-07-21T03:42:07.231284Z","iopub.status.idle":"2024-07-21T03:42:07.235478Z","shell.execute_reply.started":"2024-07-21T03:42:07.231254Z","shell.execute_reply":"2024-07-21T03:42:07.234520Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Initialize Trainer with early stopping callback\nbert_trainer = Trainer(\n    model=bert_model,\n    args=bert_training_args,\n    train_dataset=train_dataset_bert,\n    eval_dataset=val_dataset_bert,\n    data_collator=bert_data_collator,\n    compute_metrics = compute_metrics,\n    # early stopping of 2 non-improving epochs\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:45:37.940503Z","iopub.execute_input":"2024-07-21T03:45:37.941150Z","iopub.status.idle":"2024-07-21T03:45:37.952292Z","shell.execute_reply.started":"2024-07-21T03:45:37.941114Z","shell.execute_reply":"2024-07-21T03:45:37.951368Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"## BERT Training","metadata":{}},{"cell_type":"code","source":"# Train!\nbert_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:45:38.454984Z","iopub.execute_input":"2024-07-21T03:45:38.455653Z","iopub.status.idle":"2024-07-21T03:48:51.036743Z","shell.execute_reply.started":"2024-07-21T03:45:38.455623Z","shell.execute_reply":"2024-07-21T03:48:51.035850Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1756' max='2195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1756/2195 03:11 < 00:48, 9.14 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.053233</td>\n      <td>0.902183</td>\n      <td>0.911141</td>\n      <td>0.906640</td>\n      <td>0.984619</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.110800</td>\n      <td>0.043346</td>\n      <td>0.939450</td>\n      <td>0.937395</td>\n      <td>0.938421</td>\n      <td>0.989233</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026800</td>\n      <td>0.045173</td>\n      <td>0.934746</td>\n      <td>0.942612</td>\n      <td>0.938663</td>\n      <td>0.989136</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.010700</td>\n      <td>0.045328</td>\n      <td>0.933788</td>\n      <td>0.944631</td>\n      <td>0.939178</td>\n      <td>0.989525</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1756, training_loss=0.04326418232266072, metrics={'train_runtime': 191.9531, 'train_samples_per_second': 365.74, 'train_steps_per_second': 11.435, 'total_flos': 776580879783240.0, 'train_loss': 0.04326418232266072, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the test set\nresults = bert_trainer.evaluate(test_dataset_bert)\n\nprint(f\"Test set results: {results}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:49:43.980476Z","iopub.execute_input":"2024-07-21T03:49:43.980861Z","iopub.status.idle":"2024-07-21T03:49:48.946513Z","shell.execute_reply.started":"2024-07-21T03:49:43.980829Z","shell.execute_reply":"2024-07-21T03:49:48.945649Z"},"trusted":true},"execution_count":87,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test set results: {'eval_loss': 0.10282840579748154, 'eval_precision': 0.8957816377171216, 'eval_recall': 0.8948300283286119, 'eval_f1': 0.8953055801594332, 'eval_accuracy': 0.9797781845590611, 'eval_runtime': 4.954, 'eval_samples_per_second': 697.014, 'eval_steps_per_second': 21.801, 'epoch': 4.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Evaluation","metadata":{}},{"cell_type":"code","source":"# Get predictions and labels for the classification report\npredictions, labels, _ = bert_trainer.predict(test_dataset_bert)\npredictions = np.argmax(predictions, axis=-1)\n\ntrue_labels = [\n    [id2label[str(l)] for l in label if l != -100]\n    for label in labels\n]\ntrue_predictions = [\n    [id2label[str(p)] for p, l in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\n\n# Print classification report\nprint(classification_report(true_labels, true_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:51:20.596306Z","iopub.execute_input":"2024-07-21T03:51:20.597138Z","iopub.status.idle":"2024-07-21T03:51:27.051132Z","shell.execute_reply.started":"2024-07-21T03:51:20.597102Z","shell.execute_reply":"2024-07-21T03:51:27.050151Z"},"trusted":true},"execution_count":92,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         LOC       0.93      0.92      0.92      1668\n        MISC       0.78      0.78      0.78       702\n         ORG       0.86      0.88      0.87      1661\n         PER       0.96      0.94      0.95      1617\n\n   micro avg       0.90      0.89      0.90      5648\n   macro avg       0.88      0.88      0.88      5648\nweighted avg       0.90      0.89      0.90      5648\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"markdown","source":"## RoBERTa Tokenization","metadata":{}},{"cell_type":"code","source":"roberta_tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', add_prefix_space=True)\n\n# Tokenize datasets for roberta\ntrain_dataset_roberta = train_dataset.map(lambda examples: tokenize_and_align_labels(examples, roberta_tokenizer), batched=True)\nval_dataset_roberta = val_dataset.map(lambda examples: tokenize_and_align_labels(examples, roberta_tokenizer), batched=True)\ntest_dataset_roberta = test_dataset.map(lambda examples: tokenize_and_align_labels(examples, roberta_tokenizer), batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:07:16.184884Z","iopub.execute_input":"2024-07-21T04:07:16.185627Z","iopub.status.idle":"2024-07-21T04:07:19.539333Z","shell.execute_reply.started":"2024-07-21T04:07:16.185592Z","shell.execute_reply":"2024-07-21T04:07:19.538421Z"},"trusted":true},"execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f386214af4e04701a42649ce37bc1d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fab5a98c9a415c9cd4aaff11989c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed2879e6c5d4d8297404adb12916dd8"}},"metadata":{}}]},{"cell_type":"markdown","source":"# RoBERTa Model & Training Arguments","metadata":{}},{"cell_type":"code","source":"# Load DistilBERT model\nroberta_model = AutoModelForTokenClassification.from_pretrained('distilroberta-base', \n                                                             num_labels=len(ner_tags),\n                                                             id2label=id2label,\n                                                             label2id=label2id\n                                                            )","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:11:47.550495Z","iopub.execute_input":"2024-07-21T04:11:47.551369Z","iopub.status.idle":"2024-07-21T04:12:00.813523Z","shell.execute_reply.started":"2024-07-21T04:11:47.551322Z","shell.execute_reply":"2024-07-21T04:12:00.812722Z"},"trusted":true},"execution_count":97,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:  25%|##5       | 83.9M/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"243356564ddb42cbb6584bad3429ba6e"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"roberta_data_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:13:49.190424Z","iopub.execute_input":"2024-07-21T04:13:49.191189Z","iopub.status.idle":"2024-07-21T04:13:49.195356Z","shell.execute_reply.started":"2024-07-21T04:13:49.191153Z","shell.execute_reply":"2024-07-21T04:13:49.194417Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Define training arguments with early stopping\nroberta_training_args = TrainingArguments(\n    output_dir='./roberta',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:15:25.431271Z","iopub.execute_input":"2024-07-21T04:15:25.432080Z","iopub.status.idle":"2024-07-21T04:15:25.462578Z","shell.execute_reply.started":"2024-07-21T04:15:25.432042Z","shell.execute_reply":"2024-07-21T04:15:25.461652Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize Trainer with early stopping callback\nroberta_trainer = Trainer(\n    model=roberta_model,\n    args=roberta_training_args,\n    train_dataset=train_dataset_roberta,\n    eval_dataset=val_dataset_roberta,\n    data_collator=roberta_data_collator,\n    compute_metrics = compute_metrics,\n    # early stopping of 2 non-improving epochs\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:15:50.736350Z","iopub.execute_input":"2024-07-21T04:15:50.736763Z","iopub.status.idle":"2024-07-21T04:15:50.747111Z","shell.execute_reply.started":"2024-07-21T04:15:50.736730Z","shell.execute_reply":"2024-07-21T04:15:50.746302Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"## RoBERTa Training","metadata":{}},{"cell_type":"code","source":"# Train!\nroberta_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:15:51.045746Z","iopub.execute_input":"2024-07-21T04:15:51.046446Z","iopub.status.idle":"2024-07-21T04:19:09.103409Z","shell.execute_reply.started":"2024-07-21T04:15:51.046412Z","shell.execute_reply":"2024-07-21T04:19:09.102494Z"},"trusted":true},"execution_count":103,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1756' max='2195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1756/2195 03:17 < 00:49, 8.89 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.039124</td>\n      <td>0.933467</td>\n      <td>0.942107</td>\n      <td>0.937767</td>\n      <td>0.989778</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.104900</td>\n      <td>0.034562</td>\n      <td>0.946005</td>\n      <td>0.952373</td>\n      <td>0.949178</td>\n      <td>0.991394</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026800</td>\n      <td>0.035713</td>\n      <td>0.946059</td>\n      <td>0.953383</td>\n      <td>0.949707</td>\n      <td>0.991433</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.013900</td>\n      <td>0.035476</td>\n      <td>0.948028</td>\n      <td>0.954729</td>\n      <td>0.951367</td>\n      <td>0.991725</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1756, training_loss=0.04291601740417828, metrics={'train_runtime': 197.4521, 'train_samples_per_second': 355.555, 'train_steps_per_second': 11.117, 'total_flos': 755343235622430.0, 'train_loss': 0.04291601740417828, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## RoBERTa Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nresults = roberta_trainer.evaluate(test_dataset_roberta)\n\nprint(f\"RoBERTa Test set results: {results}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:19:14.475331Z","iopub.execute_input":"2024-07-21T04:19:14.475707Z","iopub.status.idle":"2024-07-21T04:19:19.349137Z","shell.execute_reply.started":"2024-07-21T04:19:14.475675Z","shell.execute_reply":"2024-07-21T04:19:19.348188Z"},"trusted":true},"execution_count":104,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"RoBERTa Test set results: {'eval_loss': 0.1058579832315445, 'eval_precision': 0.8956491592997053, 'eval_recall': 0.9148371104815864, 'eval_f1': 0.9051414557239205, 'eval_accuracy': 0.9810487778615269, 'eval_runtime': 4.8622, 'eval_samples_per_second': 710.177, 'eval_steps_per_second': 22.212, 'epoch': 4.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions and labels for the classification report\npredictions, labels, _ = roberta_trainer.predict(test_dataset_roberta)\npredictions = np.argmax(predictions, axis=-1)\n\ntrue_labels = [\n    [id2label[str(l)] for l in label if l != -100]\n    for label in labels\n]\ntrue_predictions = [\n    [id2label[str(p)] for p, l in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\n\n# Print classification report\nprint(classification_report(true_labels, true_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:19:19.351016Z","iopub.execute_input":"2024-07-21T04:19:19.351390Z","iopub.status.idle":"2024-07-21T04:19:25.866494Z","shell.execute_reply.started":"2024-07-21T04:19:19.351356Z","shell.execute_reply":"2024-07-21T04:19:25.865503Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         LOC       0.91      0.94      0.92      1668\n        MISC       0.78      0.80      0.79       702\n         ORG       0.88      0.90      0.89      1661\n         PER       0.95      0.95      0.95      1617\n\n   micro avg       0.90      0.91      0.91      5648\n   macro avg       0.88      0.90      0.89      5648\nweighted avg       0.90      0.91      0.91      5648\n\n","output_type":"stream"}]}]}